{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Reanalysis\n",
    "This notebooks saves pre-processed reanalysis data for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import glob as glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_utils import parameters as param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncep_path = \"../input_data/NCEP-NCAR-R1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 500-hPa geopotential height: \n",
    "read daily data, subtract domain average 500-hPa trend, calculate daily standardized anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgt_ncep_files = sorted(glob.glob(ncep_path+\"hgt/*.nc\"))\n",
    "\n",
    "hgt_ds = xr.open_mfdataset(hgt_ncep_files, combine = 'nested', concat_dim='time').sel(\n",
    "    lat = param.lat_bbox, \n",
    "    lon = param.lon_bbox, \n",
    "    level = param.hgt_level, \n",
    "    time = param.time_period)\n",
    "\n",
    "lats = hgt_ds['lat']\n",
    "lons = hgt_ds['lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope of hgt_trend:  0.5026036065599478 m per year\n"
     ]
    }
   ],
   "source": [
    "## calculate daily area-weighted domain average 500-hPa GPH\n",
    "area_weights = xr.broadcast(np.cos(np.deg2rad(lats)), hgt_ds, exclude = ['lat', 'time'])[0]\n",
    "hgt_domain_mean = hgt_ds['hgt'].weighted(area_weights.lat).mean(dim = ['lat', 'lon'])\n",
    "\n",
    "## calculate annual domain average 500-hPa GPH to remove seasonal variability \n",
    "hgt_domain_mean = hgt_domain_mean.groupby('time.year').mean(dim = \"time\").to_dataframe(name = \"hgt\")\n",
    "\n",
    "## calculate linear trend in 500-hPa GPH\n",
    "hgt_trend = np.polyfit(hgt_domain_mean['hgt'].index.get_level_values('year'), hgt_domain_mean['hgt'], 1)\n",
    "\n",
    "print(\"Slope of hgt_trend: \", hgt_trend[0], \"m per year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate detrended hgt\n",
    "hgt_ds['change'] = (hgt_ds.time.dt.year - 1981)*hgt_trend[0]\n",
    "hgt_ds['hgt_detrend'] = hgt_ds['hgt'] - hgt_ds['change']\n",
    "hgt_ds = hgt_ds.drop_vars('change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate standardized anomalies \n",
    "hgt_ds['mean'] = hgt_ds['hgt_detrend'].groupby('time.dayofyear').mean(dim = 'time')\n",
    "hgt_ds['sd'] = hgt_ds['hgt_detrend'].groupby('time.dayofyear').std(dim = 'time')\n",
    "hgt_ds['hgt_anom'] = (hgt_ds['hgt_detrend'].groupby('time.dayofyear') - hgt_ds['mean']).groupby('time.dayofyear')/hgt_ds['sd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgt_ds['hgt_anom'].to_netcdf(\"../processed_data/hgt_detrended_anomalies.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sea Level Pressure: \n",
    "read daily data, calculate daily standardized anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_ncep_files = sorted(glob.glob(ncep_path+\"slp/*.nc\"))\n",
    "\n",
    "slp_ds = xr.open_mfdataset(slp_ncep_files, combine = 'nested', concat_dim='time').sel(\n",
    "    lat = param.lat_bbox, \n",
    "    lon = param.lon_bbox, \n",
    "    time = param.time_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## calculate daily standardized anomalies\n",
    "slp_ds['mean'] = slp_ds['slp'].groupby('time.dayofyear').mean(dim = 'time')\n",
    "slp_ds['sd'] = slp_ds['slp'].groupby('time.dayofyear').std(dim = 'time')\n",
    "slp_ds['slp_anom'] = (slp_ds['slp'].groupby('time.dayofyear') - slp_ds['mean']).groupby('time.dayofyear')/slp_ds['sd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_ds['slp_anom'].to_netcdf(\"../processed_data/slp_anomalies.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moisture Flux: \n",
    "read uwind, vwind, and shum; calculate daily vertically integrated moisture flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "uwnd_ncep_files = sorted(glob.glob(ncep_path+\"uwnd/*.nc\"))\n",
    "vwnd_ncep_files = sorted(glob.glob(ncep_path+\"vwnd/*.nc\"))\n",
    "shum_ncep_files = sorted(glob.glob(ncep_path+\"shum/*.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_dat = xr.open_mfdataset(uwnd_ncep_files, combine = 'nested', concat_dim='time').sel(\n",
    "    lat = param.lat_bbox, lon = param.lon_bbox, time = param.time_period, \n",
    "    level = param.mflux_levels)\n",
    "\n",
    "v_dat = xr.open_mfdataset(vwnd_ncep_files, combine = 'nested', concat_dim='time').sel(\n",
    "    lat = param.lat_bbox, lon = param.lon_bbox, time = param.time_period, \n",
    "    level = param.mflux_levels)\n",
    "\n",
    "sh_dat = xr.open_mfdataset(shum_ncep_files, combine = 'nested', concat_dim='time').sel(\n",
    "    lon = param.lon_bbox, lat = param.lat_bbox, time = param.time_period, \n",
    "    level = param.mflux_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate moisture flux:** \n",
    "    \n",
    "$MF_x = \\frac{1}{g}\\int_{p_1}^{p_2}qudp$ \n",
    "\n",
    "$MF_y = \\frac{1}{g}\\int_{p_1}^{p_2}qvdp$\n",
    "\n",
    "total moisture flux $=\\sqrt{MF_x^2 + MF_y^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "moisture_flux = xr.Dataset()\n",
    "\n",
    "# multiply by -100 because pressure was in hPa instead of Pa, \n",
    "# and pressure coordinates are decreasing but we want positive integral\n",
    "moisture_flux['u_dir'] = -100*(u_dat['uwnd']*sh_dat['shum']*1/param.g).integrate(coord = 'level')\n",
    "moisture_flux['v_dir'] = -100*(v_dat['vwnd']*sh_dat['shum']*1/param.g).integrate(coord = 'level')\n",
    "moisture_flux['mag'] = np.sqrt((moisture_flux['u_dir']**2) + (moisture_flux['v_dir']**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "moisture_flux.to_netcdf(\"../processed_data/moisture_flux.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
